* Prepare stimuli
** Run code general to all blocks
  #+begin_src python :exports none :session words :results output
  import pandas as pd
  #+end_src

  #+RESULTS:

#+begin_src python :exports none :session words
  from pathlib import Path
  path_root = Path()
  path_words = path_root / ".." / "Instruments" / "Words" / "cleaned"
  path_words

  #+end_src

  #+RESULTS:
  : ../Instruments/Words/cleaned

** Spelling tests
*** Prepare words
    Prepare the words that will be used by the spelling tests.
**** Clean initial database
     This is done using stimpool which was developed for this project.

     #+begin_src python :exports both :session words :results output
       from stimpool.words import WordPool
     #+end_src

     #+RESULTS:

***** Get word pool
      This extracts the base word for each word by removing the suffix indicators.
     #+begin_src python :exports both :session words :results output
       word_pool = WordPool()
       print(word_pool.words.size)
       print(word_pool.words.head())
     #+end_src

     #+RESULTS:
     : 55457
     : 0           abs
     : 1          adsl
     : 2        adolfo
     : 3        adrián
     : 4    afganistán
     : Name: words, dtype: object

***** Select words without accented characters
      We don't want words that have accented characters because that adds a layer of complexity
      to spelling tasks because requires participants to press commands in order to type these
      characters.

      #+begin_src python :exports both :session words :results output
        word_pool.select_words_without_accented_characters()
        print(word_pool.words.size)
        print(word_pool.words.head())
      #+end_src

      #+RESULTS:
      : 46801
      : 0        abs
      : 1       adsl
      : 2     adolfo
      : 5    aguilar
      : 6      aitor
      : Name: words, dtype: object

***** Select words of more than 1 letter
      Just in case there are any

      #+begin_src python :exports both :session words :results output
        word_pool.select_words_of_length(min_len=2)
        print(word_pool.words.size)
        print(word_pool.words.head())
      #+end_src

      #+RESULTS:
      : 46794
      : 0        abs
      : 1       adsl
      : 2     adolfo
      : 5    aguilar
      : 6      aitor
      : Name: words, dtype: object

***** Save initial pool to file
        #+begin_src python :exports both :session words
          path_word_pool_initial = path_words / "1-words_pool_initial"
          word_pool.save_pool(path_word_pool_initial)
        #+end_src

        #+RESULTS:
        : None

***** Select a sample of 5,000 words
      This is sample will be analyzed in detail and then a subset will be selected.

      This sample was selected in a way that it is reproducible.

      #+begin_src python :exports both :session words :results output
        word_pool_sample = word_pool.sample_pool(n=5000)
        print(word_pool_sample.size)
        print(word_pool_sample.head())
      #+end_src

      #+RESULTS:
      : 5000
      : 6215     atestiguar
      : 40259    penalmente
      : 6817        azoemia
      : 11728        chitar
      : 31861        ladero
      : Name: words, dtype: object

***** Save the sample of words to file (5,000)
       #+begin_src python :exports both :session words :results output
         path_word_pool_n_5000 = path_words / "2-words_sample_n_5000.csv"
         word_pool_sample.to_csv(path_word_pool_n_5000, index=False, header=True)
       #+end_src

       #+RESULTS:

***** Clean pool based on meaning
      This step was done manually by the PI and 2 RAs.

      The words were reviewed to eliminate the ones that are inappropriate.
      Unappropriated words are words that meet any of the following criteria:
      - sexual content
      - negative content
      - vulgar content or otherwise not appropriate for children
      - words not used in Puerto Rico
      - Content that does not represent words (e.g., ABS)

      The process:
      - The initial word pool was copied to the file [[file:~/Ponce Health Sciences University/Project TEST - General/Instruments/Words/cleaned/3-words_analyzed_cleaned_meaning.xlsx][3a-words_analyzed_cleaned_meaning.xlsx]]
      - RAs reviewed half of the word pool and double checked the half reviewed by their
        partner. The words identified as words that should be removed were moved to the file
        [[file:~/Ponce Health Sciences University/Project TEST - General/Instruments/Words/cleaned/3-words removed.xlsx][3b-words removed.xlsx]].
      - The PI and the 2 RAs reviewed and discussed the words that were moved to the file
        [[file:~/Ponce Health Sciences University/Project TEST - General/Instruments/Words/cleaned/3-words removed.xlsx][3b-words removed.xlsx]] and identified words that should be kept. We reached a
        consensus for all words.
      - We conducted a final review of the words to be kept and the words to be
        removed. The RAs reviewed the words first and then the PI.
***** Read and clean words
      Because words were manually cleaned based on meaning, we read the words again.
      Because I spotted some duplicates in the words when reviewing them, we will
      clean them eliminating to duplicates.

      #+begin_src python :exports both :session words :results output
        path_words_cleaned_meaning_with_duplicates = path_words / "3-words_analyzed_cleaned_meaning.csv"
        words_cleaned_meaning_with_duplicates = pd.read_csv(path_words_cleaned_meaning, squeeze=True)
        words_cleaned_meaning = words_cleaned_meaning_with_duplicates.drop_duplicates()
      #+end_src

      #+RESULTS:

      The initial word pool was 5,000 words and after cleaning the words based on
      their meaning, we were able to keep
      src_python[:session words]{words_cleaned_meaning.size} {{{results(=2040=)}}} words.

**** Word difficulty
***** Analyze word difficulty
      #+begin_src python :exports both :session words :results output
        from wdiff.analyzer import Analyzer
      #+end_src

      #+RESULTS:

      #+begin_src python :exports both :session words
        analyzer = Analyzer(word_pool_sample)
        analyzer.run_all_analyses()
        word_pool_analyzed = analyzer.results
        word_pool_analyzed
      #+end_src

      #+RESULTS:
      #+begin_example
                   text  length  silent_letters  shared_phonemes  total_difficulty
      0      atestiguar      10               0                1                11
      1      penalmente      10               0                0                10
      2         azoemia       7               0                1                 8
      3          chitar       6               0                0                 6
      4          ladero       6               0                0                 6
      ...           ...     ...             ...              ...               ...
      4995      racista       7               0                2                 9
      4996      triscar       7               0                2                 9
      4997    capadocio       9               0                2                11
      4998   encanallar      10               0                2                12
      4999  zampabollos      11               0                4                15

      [5000 rows x 5 columns]
      #+end_example

**** Sample words that will be used (24 words)
     #+begin_src python :exports none :session words :results silent
       import pandas as pd
       import matplotlib as plt
     #+end_src

     These words have to be sampled guaranteeing that there are words of all
     characteristics analyzed with wdiff.

     An equal distrib for each subcategory is not guaranteed. What is guaranteed
     is that there is at least 4 words from each subcategory (i.e., meeting the
     sub-criteria).

     #+begin_src python :exports both :session words :results output
       word_pool_analyzed_sample = pd.DataFrame()

       categories_and_cutoffs = [
           ("length", 5),
           ("silent_letters", 1),
           ("shared_phonemes", 1),
       ]
       for category_and_cutoff in categories_and_cutoffs:
           # preparation
           cat = category_and_cutoff[0]
           cutoff = category_and_cutoff[1]
           data_for_cat = word_pool_analyzed[cat]

           # divide into subcats
           does_not_have_characteristic = word_pool_analyzed[data_for_cat < cutoff]
           has_characteristic = word_pool_analyzed[data_for_cat >= cutoff]

           # sample from each
           does_not_have_characteristic_sample = does_not_have_characteristic.sample(4, random_state=1)
           has_characteristic_sample = has_characteristic.sample(4, random_state=1)

           # integrate subcategories
           integrated_sample = pd.concat([does_not_have_characteristic_sample, has_characteristic_sample])

           # integrate into master
           word_pool_analyzed_sample = pd.concat([integrated_sample, word_pool_analyzed_sample])

       print(word_pool_analyzed_sample)
     #+end_src

     #+RESULTS:
     #+begin_example
                       text  length  silent_letters  shared_phonemes  total_difficulty
     2644          maronita       8               0                0                 8
     4779      enfadamiento      12               0                0                12
     2306            tramar       6               0                0                 6
     2591               erg       3               0                0                 3
     1028           augusto       7               0                1                 8
     3454       escondidijo      11               0                3                14
     485      diferenciador      13               0                1                14
     1358        prenunciar      10               0                1                11
     3068        sobrinazgo      10               0                3                13
     927            caterva       7               0                2                 9
     2453           rebufar       7               0                1                 8
     822              barca       5               0                2                 7
     2427         machaqueo       9               1                1                11
     276          halagador       9               1                0                10
     1146         contrahaz       9               1                2                12
     4903        zigzaguear      10               1                2                13
     2228              aria       4               0                0                 4
     2639              ente       4               0                0                 4
     3799              tuna       4               0                0                 4
     1258              cero       4               0                1                 5
     3178            tejero       6               0                1                 7
     2671  desgraciadamente      16               0                2                18
     2447          herbazal       8               1                2                11
     1040        contumelia      10               0                1                11
     #+end_example

 # ***** Sample 500 words for final word pool (NOT USED)
 #        #+begin_src python :exports both :session words
 #          word_pool_analyzed_sample = word_pool_analyzed.sample(500, random_state=1)
 #          path_word_pool_analyzed_sample_500 = path_words / "words_analyzed_sample_500.csv"
 #          word_pool_analyzed_sample.to_csv(path_word_pool_analyzed_sample_500, index=False)
 #          word_pool_analyzed_sample

 #        #+end_src

 #        #+RESULTS:
 #        #+begin_example
 #                     text  length  silent_letters  shared_phonemes  total_difficulty
 #        2764     jubiloso       8               0                3                11
 #        4767    tempestar       9               0                1                10
 #        3814    ajustador       9               0                2                11
 #        3499        estoy       5               0                1                 6
 #        2735  conceptismo      11               0                3                14
 #        ...           ...     ...             ...              ...               ...
 #        623     remolinar       9               0                0                 9
 #        1840     colicuar       8               0                2                10
 #        1885        bromo       5               0                1                 6
 #        4580      empalar       7               0                0                 7
 #        2048   bienquerer      10               1                2                13

 #        [500 rows x 5 columns]
 #        #+end_example
